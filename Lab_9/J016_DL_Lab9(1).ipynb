{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "J016_DL_Lab9(1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avneeshdubey99/Artificial-Intelligence-Lab/blob/master/Lab_9/J016_DL_Lab9(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCI4RERc_nS3"
      },
      "source": [
        "# TASK #1: PROJECT OVERVIEW AND BUSINESS CASE\n",
        "\n",
        "\n",
        "Let's say you're building a face recognition application and for some reason, you want the algorithm to tell you where is the corner of someone's eye. So that point has an X and Y coordinate, so you can just have a neural network have its final layer and have it just output two more numbers which I'm going to call our lx and ly to just tell you the coordinates of that corner of the person's eye."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cQRNP-_Cyed"
      },
      "source": [
        "<img src=\"https://upscfever.com/upsc-fever/en/data/deeplearning4/images/pose-detection.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBIKezc7_xOp"
      },
      "source": [
        "# TASK #2: IMPORT LIBRARIES/DATASETS AND PERFORM PRELIMINARY DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdtYpfwsw2xy",
        "scrolled": true
      },
      "source": [
        "# Import the necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from keras import optimizers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKIXGK4Ew2yG"
      },
      "source": [
        "# load the data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mjhp6dyuY5cIt78yX4VqIhBn6322FMD7' -O KeyFacialPoints.csv\n",
        "\n",
        "facialpoints_df = pd.read_csv('KeyFacialPoints.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AtcAmxGw2yN"
      },
      "source": [
        "facialpoints_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38PYj4_jCtyS"
      },
      "source": [
        "facialpoints_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSciNvhRCtyX"
      },
      "source": [
        "# Let's take a look at a sample image\n",
        "facialpoints_df['Image'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdOlt8Qkw2yR"
      },
      "source": [
        "# Since values for the image is given as space separated string, we will need to separate the values using ' ' as separator.\n",
        "# Then convert this into numpy array using np.fromstring and convert the obtained 1D array into 2D array of shape (96,96)\n",
        "facialpoints_df['Image'] = facialpoints_df['Image'].apply(lambda x: np.fromstring(x, dtype= int, sep = ' ').reshape(96,96))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbi6KB2QCtyg"
      },
      "source": [
        "# Let's obtain the shape of the resized image\n",
        "facialpoints_df['Image'][1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCqOFMHAw2yi"
      },
      "source": [
        "# Let's confirm that there are no null values \n",
        "facialpoints_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-FzZF5mCtyo"
      },
      "source": [
        "MINI CHALLENGE #1\n",
        "- Obtain the average, minimum and maximum values for 'right_eye_center_x'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDImo17BCtyo"
      },
      "source": [
        "facialpoints_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "036r_4I9BmsS"
      },
      "source": [
        "# TASK #3: PERFORM IMAGE VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJCIaUG1BIlI"
      },
      "source": [
        "# Plot a random image from the dataset along with facial keypoints. \n",
        "i = np.random.randint(1, len(facialpoints_df))\n",
        "plt.imshow(facialpoints_df['Image'][i],cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMPml1V6Ctyx"
      },
      "source": [
        "# The (x, y) coordinates for the 15 key features are plotted on top of the image\n",
        "# Below is a for loop starting from index = 1 to 32 with step of 2\n",
        "# In the first iteration j would be 1, followed by 3 and so on.\n",
        "# since x-coordinates are in even columns like 0,2,4,.. and y-coordinates are in odd columns like 1,3,5,..\n",
        "# we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.\n",
        "# in the first iteration df[i][j-1] would be df[i][0] refering the value in 1st column(x-coordinate) of the image in 'i' row.\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(facialpoints_df['Image'][i],cmap='gray')\n",
        "for j in range(1,31,2):\n",
        "        plt.plot(facialpoints_df.loc[i][j-1], facialpoints_df.loc[i][j], 'rx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FVhYoclw2ye",
        "scrolled": true
      },
      "source": [
        "import random\n",
        "\n",
        "# Let's view more images in a grid format\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(16):\n",
        "    ax = fig.add_subplot(4, 4, i + 1)    \n",
        "    image = plt.imshow(facialpoints_df['Image'][i], cmap = 'gray')\n",
        "    for j in range(1,31,2):\n",
        "        plt.plot(facialpoints_df.loc[i][j-1], facialpoints_df.loc[i][j], 'rx')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHPnv5hKCty6"
      },
      "source": [
        "MINI CHALLENGE #2\n",
        "- Plot 64 random images from the training data instead of the 16\n",
        "- HINT: You might need to choose 'random' to select random images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPP49JeACty6"
      },
      "source": [
        "import random\n",
        "\n",
        "# Let's view more images in a grid format\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(64):\n",
        "    img = np.random.randint(1, len(facialpoints_df))\n",
        "    ax = fig.add_subplot(8, 8, i + 1)    \n",
        "    image = plt.imshow(facialpoints_df['Image'][img], cmap = 'gray')\n",
        "    for j in range(1,31,2):\n",
        "        plt.plot(facialpoints_df.loc[img][j-1], facialpoints_df.loc[img][j], 'rx')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4_G8_K6LZCv"
      },
      "source": [
        "# TASK #4: PERFORM IMAGE AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07kptUMkIrKy"
      },
      "source": [
        "# Create a new copy of the dataframe\n",
        "import copy\n",
        "facialpoints_df_copy = copy.copy(facialpoints_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4JwoGNCIyfm"
      },
      "source": [
        "# obtain the header of the DataFrame (names of columns) \n",
        "\n",
        "columns = facialpoints_df_copy.columns[:-1]\n",
        "columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPbkmWU5CtzF"
      },
      "source": [
        "# Take a look at the pixel values of a sample image and see if it makes sense!\n",
        "facialpoints_df['Image'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntwasz7ZCtzI"
      },
      "source": [
        "# plot the sample image\n",
        "plt.imshow(facialpoints_df['Image'][0], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UEm9MaxCtzL"
      },
      "source": [
        "# Now Let's flip the image column horizontally \n",
        "facialpoints_df_copy['Image'] = facialpoints_df_copy['Image'].apply(lambda x: np.flip(x, axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuu3xrOJCtzP"
      },
      "source": [
        "# Now take a look at the flipped image and do a sanity check!\n",
        "# Notice that the values of pixels are now flipped\n",
        "facialpoints_df_copy['Image'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJjbPjSECtzS"
      },
      "source": [
        "# Notice that the image is flipped now\n",
        "plt.imshow(facialpoints_df_copy['Image'][0], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOu2t2u5NRJ9"
      },
      "source": [
        "# Since we are flipping the images horizontally, y coordinate values would be the same\n",
        "# X coordinate values only would need to change, all we have to do is to subtract our initial x-coordinate values from width of the image(96)\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 0:\n",
        "    facialpoints_df_copy[columns[i]] = facialpoints_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7iylFfhM_P_"
      },
      "source": [
        "# View the Original image\n",
        "plt.imshow(facialpoints_df['Image'][0],cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(facialpoints_df.loc[0][j-1], facialpoints_df.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvaGZ-lpM_c5"
      },
      "source": [
        "# View the Horizontally flipped image\n",
        "plt.imshow(facialpoints_df_copy['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(facialpoints_df_copy.loc[0][j-1], facialpoints_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubapElKTTt8n"
      },
      "source": [
        "# Concatenate the original dataframe with the augmented dataframe\n",
        "facialpoints_df_augmented = np.concatenate((facialpoints_df,facialpoints_df_copy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNOflBWyCtzi"
      },
      "source": [
        "facialpoints_df_augmented.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nggn0zuxUpTp"
      },
      "source": [
        "# Let's try to perform another image augmentation by randomly increasing images brightness\n",
        "# We multiply pixel values by random values between 1 and 2 to increase the brightness of the image\n",
        "# we clip the value between 0 and 255\n",
        "\n",
        "import random\n",
        "\n",
        "facialpoints_df_copy = copy.copy(facialpoints_df)\n",
        "facialpoints_df_copy['Image'] = facialpoints_df['Image'].apply(lambda x:np.clip(random.uniform(1, 2) * x, 0.0, 255.0))\n",
        "facialpoints_df_augmented = np.concatenate((facialpoints_df_augmented, facialpoints_df_copy))\n",
        "facialpoints_df_augmented.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGNQj6OTNiKQ"
      },
      "source": [
        "# Let's view image with increased brightness\n",
        "\n",
        "plt.imshow(facialpoints_df_copy['Image'][0], cmap = 'gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(facialpoints_df_copy.loc[0][j-1], facialpoints_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbDKCbQ9Ctzq"
      },
      "source": [
        "MINI CHALLENGE #3:\n",
        "- Augment images by flipping them vertically \n",
        "(Hint: Flip along x-axis and note that if we are flipping along x-axis, x co-ordinates won't change)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGZeqwLPCtzr"
      },
      "source": [
        "facialpoints_df_copy = copy.copy(facialpoints_df)\n",
        "\n",
        "# Flip the image column vertically (note that axis = 0) \n",
        "facialpoints_df_copy['Image'] = facialpoints_df_copy['Image'].apply(lambda x: np.flip(x, axis = 0))\n",
        "\n",
        "facialpoints_df['Image'][0]\n",
        "\n",
        "facialpoints_df_copy['Image'][0]\n",
        "\n",
        "# Since we are flipping the images vertically, x coordinate values would be the same\n",
        "# y coordinate values only would need to change, all we have to do is to subtract our initial y-coordinate values from width of the image(96)\n",
        "for i in range(len(columns)):\n",
        "  if i%2 == 1:\n",
        "    facialpoints_df_copy[columns[i]] = facialpoints_df_copy[columns[i]].apply(lambda x: 96. - float(x) )\n",
        "    \n",
        "# View the Horizontally flipped image\n",
        "plt.imshow(facialpoints_df_copy['Image'][0], cmap='gray')\n",
        "for j in range(1, 31, 2):\n",
        "        plt.plot(facialpoints_df_copy.loc[0][j-1], facialpoints_df_copy.loc[0][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH7Vt1m_s5aQ"
      },
      "source": [
        "facialpoints_df_augmented = np.concatenate((facialpoints_df_augmented, facialpoints_df_copy))\n",
        "facialpoints_df_augmented.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGZW71JOAMS"
      },
      "source": [
        "# TASK #5: PERFORM NORMALIZATION AND TRAINING DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q69GW8LqI1c9"
      },
      "source": [
        "# Obtain the value of 'Images' and normalize it\n",
        "# Note that 'Images' are in the 31st column but since indexing start from 0, we refer 31st column by 30\n",
        "img = facialpoints_df_augmented[:, 30]\n",
        "img = img/255.\n",
        "\n",
        "# Create an empty array of shape (8560, 96, 96, 1) to train the model\n",
        "X = np.empty((len(img), 96, 96, 1))\n",
        "\n",
        "# Iterate through the normalized images list and add image values to the empty array \n",
        "# Note that we need to expand it's dimension from (96,96) to (96,96,1)\n",
        "for i in range(len(img)):\n",
        "  X[i,] = np.expand_dims(img[i], axis = 2)\n",
        "\n",
        "# Convert the array type to float32\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYFNwb6aIbcG"
      },
      "source": [
        "# Obtain the values of key face points coordinates, which are to used as target.\n",
        "y = facialpoints_df_augmented[:,:30]\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430yhQLKw2y6"
      },
      "source": [
        "# Split the data into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT4d-HvkwEbi"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM6uK-ZowGXJ"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXs9BP54wHTP"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX7ZNCL1wHyY"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2fFZcqOtxh_"
      },
      "source": [
        "#Task 6 : Deep models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAK66EoLt0Yn"
      },
      "source": [
        "Model with many layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5oQw8kQ2hLe"
      },
      "source": [
        "### The identity block\n",
        "\n",
        "The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). To flesh out the different steps of what happens in a ResNet's identity block, here is an alternative diagram showing the individual steps:\n",
        "\n",
        "<img src=\"http://upscfever.com/upsc-fever/en/data/deeplearning4/images/idblock2_kiank.png\" style=\"width:650px;height:150px;\">\n",
        "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **Identity block.** Skip connection \"skips over\" 2 layers. </center></caption>\n",
        "\n",
        "The upper path is the \"shortcut path.\" The lower path is the \"main path.\" In this diagram, we have also made explicit the CONV2D and ReLU steps in each layer. To speed up training we have also added a BatchNorm step. Don't worry about this being complicated to implement--you'll see that BatchNorm is just one line of code in Keras! \n",
        "\n",
        "In this exercise, you'll actually implement a slightly more powerful version of this identity block, in which the skip connection \"skips over\" 3 hidden layers rather than 2 layers. It looks like this: \n",
        "\n",
        "<img src=\"http://upscfever.com/upsc-fever/en/data/deeplearning4/images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
        "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Identity block.** Skip connection \"skips over\" 3 layers.</center></caption>\n",
        "\n",
        "Here're the individual steps.\n",
        "\n",
        "First component of main path: \n",
        "- The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. Use 0 as the seed for the random initialization. \n",
        "- The first BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2a'`.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "\n",
        "Second component of main path:\n",
        "- The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is \"same\" and its name should be `conv_name_base + '2b'`. Use 0 as the seed for the random initialization. \n",
        "- The second BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2b'`.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "\n",
        "Third component of main path:\n",
        "- The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2c'`. Use 0 as the seed for the random initialization. \n",
        "- The third BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component. \n",
        "\n",
        "Final step: \n",
        "- The shortcut and the input are added together.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "\n",
        "**Exercise**: Implement the ResNet identity block. We have implemented the first component of the main path. Please read over this carefully to make sure you understand what it is doing. You should implement the rest. \n",
        "- To implement the Conv2D step: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
        "- To implement BatchNorm: [See reference](https://faroit.github.io/keras-docs/1.2.2/layers/normalization/) (axis: Integer, the axis that should be normalized (typically the channels axis))\n",
        "- For the activation, use:  `Activation('relu')(X)`\n",
        "- To add the value passed forward by the shortcut: [See reference](https://keras.io/layers/merge/#add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4G5kYAl1FYt"
      },
      "source": [
        "## The convolutional block\n",
        "\n",
        "You've implemented the ResNet identity block. Next, the ResNet \"convolutional block\" is the other type of block. You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path: \n",
        "\n",
        "<img src=\"http://upscfever.com/upsc-fever/en/data/deeplearning4/images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
        "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Convolutional block** </center></caption>\n",
        "\n",
        "The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix $W_s$ discussed in lecture.) For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. \n",
        "\n",
        "The details of the convolutional block are as follows. \n",
        "\n",
        "First component of main path:\n",
        "- The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. \n",
        "- The first BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2a'`.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "\n",
        "Second component of main path:\n",
        "- The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is \"same\" and it's name should be `conv_name_base + '2b'`.\n",
        "- The second BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2b'`.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "\n",
        "Third component of main path:\n",
        "- The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is \"valid\" and it's name should be `conv_name_base + '2c'`.\n",
        "- The third BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component. \n",
        "\n",
        "Shortcut path:\n",
        "- The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is \"valid\" and its name should be `conv_name_base + '1'`.\n",
        "- The BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '1'`. \n",
        "\n",
        "Final step: \n",
        "- The shortcut and the main path values are added together.\n",
        "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
        "    \n",
        "**Exercise**: Implement the convolutional block. We have implemented the first component of the main path; you should implement the rest. As before, always use 0 as the seed for the random initialization, to ensure consistency with our grader.\n",
        "- [Conv Hint](https://keras.io/layers/convolutional/#conv2d)\n",
        "- [BatchNorm Hint](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
        "- For the activation, use:  `Activation('relu')(X)`\n",
        "- [Addition Hint](https://keras.io/layers/merge/#add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plU9NVY2QHtk"
      },
      "source": [
        "# TASK #7: BUILD DEEP RESIDUAL NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-1c26gotLc"
      },
      "source": [
        "def res_block(X, filter, stage):\n",
        "    \n",
        "  # CONVOLUTIONAL BLOCK\n",
        "  X_copy = X\n",
        "  f1 , f2, f3 = filter\n",
        "\n",
        "  # Main Path\n",
        "  #Layer L+1\n",
        "  X = Conv2D(f1, (1,1), strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = MaxPool2D((2,2))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  #Layer L+2\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  #Layer L+3\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n",
        "\n",
        "  # Short path\n",
        "  X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n",
        "  X_copy = MaxPool2D((2,2))(X_copy)\n",
        "  X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n",
        "\n",
        "  # Add data from main and short paths\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "    \n",
        "    \n",
        "  # IDENTITY BLOCK 1\n",
        "  X_copy = X\n",
        "    \n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n",
        "\n",
        "  # Add both paths together (Note that we feed the original input as is hence the name \"identity\")\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "    \n",
        "    \n",
        "  # IDENTITY BLOCK 2\n",
        "  X_copy = X\n",
        "\n",
        "  # Main Path\n",
        "  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n",
        "  X = Activation('relu')(X) \n",
        "\n",
        "  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n",
        "\n",
        "  # Add both paths together (Note that we feed the original input as is hence the name \"identity\")\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weamw45uNl58"
      },
      "source": [
        "#Residual networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vs7m8W91JYS"
      },
      "source": [
        "#if your mini-batch is a matrix A mxn, i.e. m samples and n features, the normalization axis should be axis=0. \n",
        "#As your said, what we want is to normalize every feature individually, the default axis = -1 in keras \n",
        "#because when it is used in the convolution-layer, \n",
        "#the dimensions of figures dataset are usually (samples, width, height, channal), and the batch samples \n",
        "#are normalized long the channal axis(the last axis).\n",
        "\n",
        "input_shape = (96,96,1)\n",
        "\n",
        "# Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "# Zero-padding\n",
        "X = ZeroPadding2D((3,3))(X_input)\n",
        "\n",
        "# Stage #1\n",
        "X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
        "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((3,3), strides= (2,2))(X)\n",
        "\n",
        "# Stage #2\n",
        "X = res_block(X, filter= [64,64,256], stage= 2)\n",
        "\n",
        "# Stage #3\n",
        "X = res_block(X, filter= [128,128,512], stage= 3)\n",
        "\n",
        "\n",
        "# Average Pooling\n",
        "X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n",
        "\n",
        "# Final layer\n",
        "X = Flatten()(X)\n",
        "X = Dense(4096, activation = 'relu')(X)\n",
        "X = Dropout(0.2)(X)\n",
        "X = Dense(2048, activation = 'relu')(X)\n",
        "X = Dropout(0.1)(X)\n",
        "X = Dense(30, activation = 'relu')(X)\n",
        "\n",
        "\n",
        "model = Model( inputs= X_input, outputs = X)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAAj64V1RaXk"
      },
      "source": [
        "# TASK #8: COMPILE AND TRAIN DEEP LEARNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfl9pCQQQlPe"
      },
      "source": [
        "adam = tf.keras.optimizers.Adam(lr = 0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss=\"mean_squared_error\", optimizer = adam, metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArX64njzOPHt"
      },
      "source": [
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"weights.hdf5\", verbose = 1, save_best_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35ckVHHPgcb"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 256, epochs= 100, validation_split = 0.05, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG9P_GkkCt0P"
      },
      "source": [
        "# Save trained model\n",
        "model_json = model.to_json()\n",
        "with open('KeyPointDetector.json', 'w') as json_file:\n",
        "        json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz6VHutDRoni"
      },
      "source": [
        "# TASK #9: ASSESS TRAINED MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWUXbG34Ct0U"
      },
      "source": [
        "# instead of training from scratch, you can load trained model weights\n",
        "with open('KeyPointDetector.json', 'r') as json_file:\n",
        "    json_SavedModel = json_file.read()\n",
        "model = tf.keras.models.model_from_json(json_SavedModel)\n",
        "model.load_weights('weights.hdf5')\n",
        "model.compile(loss=\"mean_squared_error\", optimizer = adam, metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em-MPopu7QUi"
      },
      "source": [
        "# saving the model in tensorflow format\n",
        "model.save('./MyModel_tf',save_format='tf')\n",
        "\n",
        "\n",
        "# loading the saved model\n",
        "loaded_model = tf.keras.models.load_model('./MyModel_tf')\n",
        "\n",
        "# retraining the model\n",
        "loaded_model.fit(X_train, y_train, batch_size = 256, epochs= 100, validation_split = 0.05, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv0VrvySPpzl"
      },
      "source": [
        "# Evaluate trained model\n",
        "result = model.evaluate(X_test,y_test)\n",
        "print(\"Accuracy : {}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGA0O-yEl0Gt"
      },
      "source": [
        "# Make prediction using the testing dataset\n",
        "df_predict = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvvsEkdKWNWV"
      },
      "source": [
        "# Print the rmse loss values\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "rms = sqrt(mean_squared_error(y_test, df_predict))\n",
        "print(\"RMSE value : {}\".format(rms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT0h-JadTbeJ"
      },
      "source": [
        "# Convert the predicted values into a dataframe\n",
        "\n",
        "df_predict= pd.DataFrame(df_predict, columns = columns)\n",
        "df_predict.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afMa9D2rTJye"
      },
      "source": [
        "# Plot the test images and their predicted keypoints\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(8):\n",
        "    ax = fig.add_subplot(4, 2, i + 1)\n",
        "    # Using squeeze to convert the image shape from (96,96,1) to (96,96)\n",
        "    plt.imshow(X_test[i].squeeze(),cmap='gray')\n",
        "    for j in range(1,31,2):\n",
        "            plt.plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDX06A93S_8T"
      },
      "source": [
        "# CONGRATULATIONS ON FINISHING THE PROJECT"
      ]
    }
  ]
}